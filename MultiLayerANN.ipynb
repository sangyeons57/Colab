{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvzvld8AgBfXowU43GX5RK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sangyeons57/Colab/blob/main/MultiLayerANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47784291"
      },
      "source": [
        "# 필요한 라이브러리 import\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer:\n",
        "    def forward(self, input, training = True):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def backward(self, grad_out):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def params(self):\n",
        "        return []"
      ],
      "metadata": {
        "id": "HsSbeuqX1csx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearLayer(Layer):\n",
        "    def __init__(self, input_count, output_count, bias=True):\n",
        "        self.W = np.random.randn(input_count, output_count) * np.sqrt(2.0 / (input_count))\n",
        "        self.use_bias = bool(bias)\n",
        "        if self.use_bias:\n",
        "          self.b = np.zeros((1,output_count))\n",
        "        else:\n",
        "            self.b = None\n",
        "\n",
        "        self.dW= None\n",
        "        self.db= None\n",
        "        self.input = None\n",
        "\n",
        "    def forward(self, input, training=True):\n",
        "        self.input = input\n",
        "        output = input @ self.W\n",
        "        if self.use_bias:\n",
        "            output += self.b\n",
        "        return output\n",
        "\n",
        "    def backward(self, grad_out):\n",
        "        self.dW = self.input.T @ grad_out\n",
        "        if self.use_bias:\n",
        "            self.db = grad_out.sum(axis=0, keepdims=True)\n",
        "        else :\n",
        "            self.db = None\n",
        "\n",
        "        return grad_out @ self.W.T\n",
        "\n",
        "    def params(self):\n",
        "        params = [(self.W, self.dW)]\n",
        "        if self.use_bias:\n",
        "            params.append((self.b, self.db))\n",
        "        return params\n"
      ],
      "metadata": {
        "id": "oK5F_IBY5OiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Activation function\n",
        "class ReLU(Layer):\n",
        "    def forward(self, x, training=True):\n",
        "        self.mask = (x > 0)\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0\n",
        "        return out\n",
        "\n",
        "    def backward(self, grad_out):\n",
        "        grad_out[self.mask] = 0\n",
        "        dx = grad_out\n",
        "        return dx"
      ],
      "metadata": {
        "id": "jUtNbjmY2CJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Layer Container\n",
        "'''\n",
        "*layers 여러개의 위치 인자를 전달 가능한 객체\n",
        "'''\n",
        "class Sequential(Layer):\n",
        "    def __init__(self, *layers):\n",
        "        self.layers = list(layers)\n",
        "\n",
        "    def forward(self, input, training = True):\n",
        "        for layer in self.layers:\n",
        "            input = layer.forward(input, training=training)\n",
        "        return input\n",
        "\n",
        "    def backward(self, grad_out):\n",
        "        for layer in reversed(self.layers):\n",
        "            grad_out = layer.backward(grad_out)\n",
        "        return grad_out\n",
        "\n",
        "    def params(self):\n",
        "        params = []\n",
        "        for layer in self.layers:\n",
        "            params += layer.params()\n",
        "        return params"
      ],
      "metadata": {
        "id": "K_JPQtUN1o0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    x -= np.max(x, axis=1, keepdims=True)\n",
        "    e = np.exp(x)\n",
        "    return e / np.sum(e, axis=1, keepdims=True)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
        "\n",
        "#오차값\n",
        "class Loss:\n",
        "    def __init__(self):\n",
        "        self.params = {}\n",
        "        self.grads = {}\n",
        "\n",
        "    def cross_entropy_forward(self, logits, y_true):\n",
        "        self.probs = softmax(logits)\n",
        "        self.y_true = y_true\n",
        "        return -np.sum(y_true * np.log(self.probs + 1e-8)) / logits.shape[0]\n",
        "\n",
        "    def cross_entropy_backward(self):\n",
        "        return (self.probs - self.y_true) / self.probs.shape[0]"
      ],
      "metadata": {
        "id": "q6GNVIQo2jt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimzier\n",
        "class SGD:\n",
        "    def __init__(self, lr = 0.01, decay=0.0):\n",
        "        self.init_lr = lr\n",
        "        self.lr = lr\n",
        "        self.decay = decay\n",
        "        self.interations = 0\n",
        "\n",
        "    def step(self, params):\n",
        "        self.interations += 1\n",
        "        self.lr = self.init_lr / (1 + self.decay * self.interations)\n",
        "        for p, dp in params:\n",
        "            p -= self.lr * dp\n",
        "\n"
      ],
      "metadata": {
        "id": "zen8oFoK2r1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model:\n",
        "    def __init__(self, net : Sequential, loss_fn: Loss, optimizer: SGD):\n",
        "        self.net = net\n",
        "        self.loss_fn = loss_fn\n",
        "        self.optimizer = optimizer\n",
        "        self.training = True\n",
        "\n",
        "    def train(self):\n",
        "        self.training = True\n",
        "\n",
        "    def eval(self):\n",
        "        self.training = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net.forward(x, training=self.training)\n",
        "\n",
        "    def train_step(self, x_batch, y_batch):\n",
        "        self.train()\n",
        "        logits = self.forward(x_batch) #순전파\n",
        "        loss = self.loss_fn.cross_entropy_forward(logits, y_batch) # 오차 구하기\n",
        "        grad = self.loss_fn.cross_entropy_backward() #오차 역전파\n",
        "        self.net.backward(grad) # 신경망에 역전파\n",
        "        self.optimizer.step(self.net.params()) # 기울기에(오차에 대한 영향력) 따른 수정 적용\n",
        "        return loss\n",
        "\n",
        "    def predict(self, x):\n",
        "        self.eval()\n",
        "        logits = self.forward(x)\n",
        "        return np.argmax(logits, axis=1)"
      ],
      "metadata": {
        "id": "XiZySX4c1tYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3RcRGTba2jOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dropout(Layer):\n",
        "    def __init__(self, rate=0.5):\n",
        "        self.rate = rate\n",
        "\n",
        "    def forward(self, input, training=True):\n",
        "        if training:\n",
        "            self.mask = (np.random.rand(*input.shape) > self.rate) * (1.0 - self.rate)\n",
        "            return input * self.mask\n",
        "        else:\n",
        "            return input\n",
        "\n",
        "    def backward(self, grad_out):\n",
        "        return grad_out * getattr(self, \"mask\", 1) # training=False 인 경우 mask값이 없어서 들어온 값 그대로 출력\n",
        "\n"
      ],
      "metadata": {
        "id": "NUUm8swRLhgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 배치 정규화\n",
        "# 학습 안정화\n",
        "class BatchNorm(Layer):\n",
        "    def __init__(self, dim, momentum = 0.9, eps=1e-5 ):\n",
        "        self.gamma = np.ones((1,dim))\n",
        "        self.beta = np.zeros((1,dim))\n",
        "        self.momentum = momentum\n",
        "        self.eps = eps\n",
        "\n",
        "        self.running_mean = np.zeros((1,dim))\n",
        "        self.running_var = np.ones((1,dim))\n",
        "\n",
        "        self.dgama= None\n",
        "        self.dbeta= None\n",
        "\n",
        "        self.x_centered = None\n",
        "        self.x_norm = None\n",
        "        self.std = None\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, input, training=True):\n",
        "        if training:\n",
        "            mean = input.mean(axis=0, keepdims=True)\n",
        "            var = input.var(axis=0, keepdims=True)\n",
        "\n",
        "            self.x_centered = input - mean\n",
        "            self.std = np.sqrt(var + self.eps)\n",
        "            self.x_norm = self.x_centered / self.std\n",
        "            out = self.gamma * self.x_norm + self.beta\n",
        "\n",
        "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mean\n",
        "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * var\n",
        "            return out\n",
        "        else:\n",
        "            input_norm = (input - self.running_mean) / np.sqrt(self.running_var + self.eps)\n",
        "            return self.gamma * input_norm + self.beta\n",
        "\n",
        "    def backward(self, grad_out):\n",
        "        m = grad_out.shape[0]\n",
        "\n",
        "\n",
        "        # Parameter gradients\n",
        "        self.dgamma = (grad_out * self.x_norm).sum(axis=0, keepdims=True)\n",
        "        self.dbeta = grad_out.sum(axis=0, keepdims=True)\n",
        "\n",
        "        # Gradient through scale and shift\n",
        "        dx_norm = grad_out * self.gamma  # (N, D)\n",
        "\n",
        "        # Intermediate partials for mean/variance\n",
        "        dvar = (dx_norm * self.x_centered * -0.5 * (self.std ** -3)).sum(axis=0, keepdims=True)\n",
        "        dmu = (dx_norm * -1.0 / self.std).sum(axis=0, keepdims=True) + dvar * (-2.0 * self.x_centered.mean(axis=0, keepdims=True))\n",
        "\n",
        "        # Final gradient w.r.t. input\n",
        "        dx = dx_norm / self.std + dvar * 2.0 * self.x_centered / m + dmu / m\n",
        "        return dx\n",
        "\n",
        "    def params(self):\n",
        "        return [(self.gamma, self.dgamma), (self.beta, self.dbeta)]"
      ],
      "metadata": {
        "id": "fYz57_HxO70E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# ─── 1. 데이터 로드 및 전처리 ───────────────────────────────────\n",
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
        "# (60000,28,28), (10000,28,28)\n",
        "\n",
        "# 28x28 이미지 플랫화 및 0~1 정규화\n",
        "train_X = train_X.reshape(-1, 784) / 255.0\n",
        "test_X  = test_X.reshape(-1, 784) / 255.0\n",
        "\n",
        "# 원핫 인코딩\n",
        "num_classes = 10\n",
        "train_Y = np.eye(num_classes)[train_y]\n",
        "test_Y  = np.eye(num_classes)[test_y]\n",
        "\n",
        "# ─── 2. 모델 구성 ───────────────────────────────────────────────\n",
        "# net = Sequential(\n",
        "#     LinearLayer(784, 128),\n",
        "#     ReLU(),\n",
        "#     LinearLayer(128, num_classes)\n",
        "# )\n",
        "\n",
        "net = Sequential(\n",
        "    LinearLayer(784, 256, bias=False),      # bias 생략: BN으로 대체\n",
        "    BatchNorm(256, momentum=0.9),           # 최근 배치 중심\n",
        "    ReLU(),\n",
        "    Dropout(rate=0.1),\n",
        "\n",
        "    LinearLayer(256, 128, bias=False),\n",
        "    BatchNorm(128, momentum=0.9),\n",
        "    ReLU(),\n",
        "    Dropout(rate=0.1),\n",
        "\n",
        "    LinearLayer(128, num_classes)\n",
        ")\n",
        "\n",
        "loss_fn = Loss()\n",
        "optimizer = SGD(lr=0.5, decay=0.001)\n",
        "model = Model(net, loss_fn, optimizer)\n",
        "\n",
        "# ─── 3. 학습 루프 (에폭 & 배치) ─────────────────────────────────\n",
        "batch_size = 128\n",
        "epochs = 3\n",
        "num_train = train_X.shape[0]\n",
        "steps = num_train // batch_size\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # 데이터 셔플\n",
        "    idx = np.random.permutation(num_train)\n",
        "    Xs, Ys = train_X[idx], train_Y[idx]\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for i in range(steps):\n",
        "        start = i * batch_size\n",
        "        xb = Xs[start:start+batch_size]\n",
        "        yb = Ys[start:start+batch_size]\n",
        "        epoch_loss += model.train_step(xb, yb)\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f\"Step {i+1}/{steps}, loss={epoch_loss:.4f}\")\n",
        "        elif (i == 0):\n",
        "            print(f\"First Step, loss={epoch_loss:.4f}\")\n",
        "    epoch_loss /= steps\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, loss={epoch_loss:.4f}\")\n",
        "\n",
        "# ─── 4. 테스트 정확도 평가 ────────────────────────────────────\n",
        "preds = model.predict(test_X)\n",
        "accuracy = (preds == test_y).mean()\n",
        "print(\"Test accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQXOebwhXq9N",
        "outputId": "919bf10e-6814-4e18-8f88-ca9eb636fc82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Step, loss=2.7464\n",
            "Step 100/468, loss=40.6903\n",
            "Step 200/468, loss=61.6069\n",
            "Step 300/468, loss=79.4382\n",
            "Step 400/468, loss=95.1625\n",
            "Epoch 1/3, loss=0.2255\n",
            "First Step, loss=0.1308\n",
            "Step 100/468, loss=11.8644\n",
            "Step 200/468, loss=22.5752\n",
            "Step 300/468, loss=34.1272\n",
            "Step 400/468, loss=44.1861\n",
            "Epoch 2/3, loss=0.1079\n",
            "First Step, loss=0.1066\n",
            "Step 100/468, loss=7.5760\n",
            "Step 200/468, loss=15.5798\n",
            "Step 300/468, loss=23.8533\n",
            "Step 400/468, loss=32.1438\n",
            "Epoch 3/3, loss=0.0794\n",
            "Test accuracy: 0.9759\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 현제 최대 96.8%\n",
        "# 신규 최대 97.59%\n",
        "# 내일 학습룰 99% 도전 ELU, CNN,BN 을 활용해서"
      ],
      "metadata": {
        "id": "ZgIO0x72fcaN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}